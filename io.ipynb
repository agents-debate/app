{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWVz_1H-rovp",
        "outputId": "db8fcbe1-e629-4848-8ad5-ee23b8282af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-5d91d8418e09>:190: UserWarning:\n",
            "\n",
            "The palette list has more values (4) than needed (3), which may not be intended.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insights report saved to: /content/drive/MyDrive/output_anxiety_static_viz/insights_report.md\n",
            "Execution completed successfully - Static Visualization Enhanced Notebook with integrated insights report.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Enhanced Anxiety Intervention Analysis with Static Visualizations and LLM Insights\n",
        "\n",
        "This notebook analyzes a synthetic dataset of an anxiety intervention, focusing on\n",
        "creating static visualizations using Matplotlib and Seaborn to explore the data and gain\n",
        "insights. It incorporates SHAP values for feature importance and a\n",
        "static hypergraph visualization using NetworkX. It generates insights\n",
        "using a built-in analysis engine rather than external API calls.\n",
        "\n",
        "Workflow:\n",
        "1. Data Loading and Validation: Load and validate the synthetic dataset.\n",
        "2. Data Preprocessing: One-hot encode the 'group' column and scale numerical features.\n",
        "3. SHAP Value Analysis: Calculate and visualize SHAP values.\n",
        "4. Static Visualizations: Create static Matplotlib/Seaborn plots (KDE, Violin, Parallel Coordinates).\n",
        "5. Hypergraph Visualization: Create a static hypergraph using NetworkX.\n",
        "6. Statistical Analysis: Perform bootstrap analysis for confidence intervals.\n",
        "7. Insights Report: Generate a comprehensive insights report based on statistical findings.\n",
        "\n",
        "Keywords: Anxiety Intervention, Static Visualization, Matplotlib, Seaborn, SHAP, Hypergraph, NetworkX,\n",
        "Explainability, Data Visualization, Machine Learning, Statistical Analysis\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import shap\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "# from plotly.express import px  # Removed Plotly\n",
        "# import plotly.graph_objects as go  # Removed Plotly\n",
        "from scipy.stats import bootstrap\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import datetime\n",
        "\n",
        "# Suppress warnings (with caution - better to handle specific warnings)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"plotly\") # Removed Plotly warning\n",
        "\n",
        "# Google Colab environment check\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Constants\n",
        "OUTPUT_PATH = \"./output_anxiety_static_viz/\" if not COLAB_ENV else \"/content/drive/MyDrive/output_anxiety_static_viz/\"\n",
        "PARTICIPANT_ID_COLUMN = \"participant_id\"\n",
        "GROUP_COLUMN = \"group\"  # Original group column\n",
        "ANXIETY_PRE_COLUMN = \"anxiety_pre\"\n",
        "ANXIETY_POST_COLUMN = \"anxiety_post\"\n",
        "LINE_WIDTH = 2.5\n",
        "BOOTSTRAP_RESAMPLES = 500\n",
        "AUTHOR = \"Claude\"  # Author identifier\n",
        "\n",
        "# --- DDQN Agent Class --- (Remains unchanged, as it's a placeholder)\n",
        "class DDQNAgent:\n",
        "    \"\"\"\n",
        "    A simplified DDQN agent for demonstration purposes. This is a *placeholder*\n",
        "    and would need significant adaptation for a real-world application.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        # Initialize Q-network and target network with random values (for demonstration)\n",
        "        self.q_network = np.random.rand(state_dim, action_dim)\n",
        "        self.target_network = np.copy(self.q_network)\n",
        "\n",
        "    def act(self, state, epsilon=0.01):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.action_dim)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_network[state])  # Exploit\n",
        "\n",
        "    def learn(self, batch, gamma=0.99, learning_rate=0.1):\n",
        "        \"\"\"Placeholder learning function. A real implementation would update the Q-network.\"\"\"\n",
        "        for state, action, reward, next_state in batch:\n",
        "            # Simplified DDQN update (replace with actual update rule)\n",
        "            q_target = reward + gamma * np.max(self.target_network[next_state])\n",
        "            q_predict = self.q_network[state, action]\n",
        "            self.q_network[state, action] += learning_rate * (q_target - q_predict)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Placeholder target network update.\"\"\"\n",
        "        self.target_network = np.copy(self.q_network)\n",
        "\n",
        "# --- Functions ---\n",
        "def create_output_directory(path):\n",
        "    \"\"\"Creates the output directory if it doesn't exist, handling errors.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        return True\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating output directory: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_data_from_synthetic_string(csv_string: str) -> pd.DataFrame:\n",
        "    \"\"\"Loads data from a CSV-formatted string, handling errors.\"\"\"\n",
        "    try:\n",
        "        csv_file = StringIO(csv_string)\n",
        "        return pd.read_csv(csv_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None # Return empty DataFrame on error\n",
        "\n",
        "def validate_dataframe(df: pd.DataFrame, required_columns: list) -> bool:\n",
        "    \"\"\"Validates if the DataFrame contains the necessary columns and data types.\"\"\"\n",
        "    if df is None:\n",
        "        print(\"Error: DataFrame is None. Cannot validate.\")\n",
        "        return False, None\n",
        "\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Error: Missing columns: {missing_columns}\")\n",
        "        return False, None\n",
        "\n",
        "    if not pd.api.types.is_numeric_dtype(df[ANXIETY_PRE_COLUMN]):\n",
        "        print(f\"Error: {ANXIETY_PRE_COLUMN} must be numeric.\")\n",
        "        return False, None\n",
        "    if not pd.api.types.is_numeric_dtype(df[ANXIETY_POST_COLUMN]):\n",
        "        print(f\"Error: {ANXIETY_POST_COLUMN} must be numeric.\")\n",
        "        return False, None\n",
        "\n",
        "    if df[PARTICIPANT_ID_COLUMN].duplicated().any():\n",
        "        print(\"Error: Duplicate participant IDs found.\")\n",
        "        return False, None\n",
        "\n",
        "    valid_groups = [\"Group A\", \"Group B\", \"Control\"]\n",
        "    if not df[GROUP_COLUMN].isin(valid_groups).all():\n",
        "        print(f\"Error: Invalid group values. Must be one of: {valid_groups}\")\n",
        "        return False, None\n",
        "\n",
        "    return True, valid_groups\n",
        "\n",
        "def scale_data(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
        "    \"\"\"Scales specified columns of a DataFrame to the range [0, 1], handling errors.\"\"\"\n",
        "    try:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[columns] = scaler.fit_transform(df[columns])\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error scaling data: {e}\")\n",
        "        return None  # Return None on error\n",
        "\n",
        "def calculate_shap_values(df: pd.DataFrame, feature_columns: list, target_column: str, output_path: str) -> str:\n",
        "    \"\"\"Calculates and visualizes SHAP values, handling errors.\"\"\"\n",
        "    try:\n",
        "        model_rf = RandomForestRegressor(random_state=42).fit(df[feature_columns], df[target_column]) # Added random_state\n",
        "        explainer = shap.TreeExplainer(model_rf)\n",
        "        shap_values = explainer.shap_values(df[feature_columns])\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.style.use('dark_background')\n",
        "        shap.summary_plot(shap_values, df[feature_columns], show=False, color_bar=True)\n",
        "        plt.savefig(os.path.join(output_path, 'shap_summary.png'))\n",
        "        plt.close()\n",
        "        return f\"SHAP summary for features {feature_columns} predicting {target_column}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in SHAP analysis: {e}\")\n",
        "        return f\"Error in SHAP analysis: {e}\"\n",
        "\n",
        "def create_kde_plot(df: pd.DataFrame, column1: str, column2: str, output_path: str, colors: list) -> str:\n",
        "    \"\"\"Creates a KDE plot using Seaborn, handling errors.\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.kdeplot(df[column1], color=colors[0], label=column1.capitalize(), fill=True)\n",
        "        sns.kdeplot(df[column2], color=colors[1], label=column2.capitalize(), fill=True)\n",
        "        plt.title('KDE Plot of Anxiety Levels')\n",
        "        plt.xlabel('Anxiety Level')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(output_path, 'kde_plot.png'))\n",
        "        plt.close()\n",
        "        return f\"KDE plot visualizing distributions of {column1} and {column2}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating KDE plot: {e}\")\n",
        "        return \"Error creating KDE plot.\"\n",
        "\n",
        "def create_violin_plot(df: pd.DataFrame, group_column: str, y_column: str, output_path: str, colors: list) -> str:\n",
        "    \"\"\"Creates a violin plot using Seaborn, handling errors.\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.violinplot(x=group_column, y=y_column, data=df, palette=colors)\n",
        "        plt.title('Violin Plot of Anxiety Distribution by Group')\n",
        "        plt.xlabel('Group')\n",
        "        plt.ylabel(y_column.capitalize())\n",
        "        plt.savefig(os.path.join(output_path, 'violin_plot.png'))\n",
        "        plt.close()\n",
        "        return f\"Violin plot showing {y_column} across {group_column}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating violin plot: {e}\")\n",
        "        return \"Error creating violin plot.\"\n",
        "\n",
        "def create_parallel_coordinates_plot(df: pd.DataFrame, group_column: str, anxiety_pre_column: str, anxiety_post_column: str, output_path: str, colors: list) -> str:\n",
        "    \"\"\"Creates a parallel coordinates plot using Matplotlib, handling errors.\"\"\"\n",
        "    try:\n",
        "        plot_df = df[[group_column, anxiety_pre_column, anxiety_post_column]].copy()\n",
        "        unique_groups = plot_df[group_column].unique()\n",
        "        group_color_map = {group: colors[i % len(colors)] for i, group in enumerate(unique_groups)}\n",
        "        plot_df['color'] = plot_df[group_column].map(group_color_map)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for group in unique_groups:\n",
        "            group_data = plot_df[plot_df[group_column] == group]\n",
        "            for _, row in group_data.iterrows():\n",
        "                plt.plot([0, 1], [row[anxiety_pre_column], row[anxiety_post_column]], color=row['color'])\n",
        "\n",
        "        plt.xticks([0, 1], [anxiety_pre_column.capitalize(), anxiety_post_column.capitalize()])\n",
        "        plt.title(\"Parallel Coordinates Plot: Anxiety Levels Pre- vs Post-Intervention by Group\")\n",
        "        plt.ylabel(\"Anxiety Level (Scaled)\")\n",
        "\n",
        "        # Create custom legend\n",
        "        legend_elements = [plt.Line2D([0], [0], color=color, lw=2, label=group) for group, color in group_color_map.items()]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        plt.savefig(os.path.join(output_path, 'parallel_coordinates_plot.png'))\n",
        "        plt.close()\n",
        "        return \"Parallel coordinates plot of anxiety pre vs post intervention by group\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating parallel coordinates plot: {e}\")\n",
        "        return \"Error creating parallel coordinates plot.\"\n",
        "\n",
        "def visualize_hypergraph(df: pd.DataFrame, anxiety_pre_column: str, anxiety_post_column: str, output_path: str, colors: list) -> str:\n",
        "    \"\"\"Creates a hypergraph visualization using NetworkX (static image), handling errors.\"\"\"\n",
        "    try:\n",
        "        G = nx.Graph()\n",
        "        participant_ids = df[PARTICIPANT_ID_COLUMN].tolist()\n",
        "        G.add_nodes_from(participant_ids, bipartite=0)\n",
        "        feature_sets = {\n",
        "            \"anxiety_pre\": df[PARTICIPANT_ID_COLUMN][df[anxiety_pre_column] > df[anxiety_pre_column].mean()].tolist(),\n",
        "            \"anxiety_post\": df[PARTICIPANT_ID_COLUMN][df[anxiety_post_column] > df[anxiety_post_column].mean()].tolist()\n",
        "        }\n",
        "        feature_nodes = list(feature_sets.keys())\n",
        "        G.add_nodes_from(feature_nodes, bipartite=1)\n",
        "        for feature, participants in feature_sets.items():\n",
        "            for participant in participants:\n",
        "                G.add_edge(participant, feature)\n",
        "        pos = nx.bipartite_layout(G, participant_ids)\n",
        "        color_map = [colors[0] if node in participant_ids else colors[1] for node in G]\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.style.use('dark_background')\n",
        "        nx.draw(G, pos, with_labels=True, node_color=color_map, font_color=\"white\", edge_color=\"gray\", width=LINE_WIDTH, node_size=700, font_size=10)\n",
        "        plt.title(\"Hypergraph Representation of Anxiety Patterns\", color=\"white\") # Title adjusted\n",
        "        plt.savefig(os.path.join(output_path, \"hypergraph.png\")) # Filename adjusted\n",
        "        plt.close()\n",
        "        return \"Hypergraph visualizing participant relationships\" # Description adjusted\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating hypergraph: {e}\")\n",
        "        return \"Error creating hypergraph.\"\n",
        "\n",
        "def perform_bootstrap(data: pd.Series, statistic: callable, n_resamples: int = BOOTSTRAP_RESAMPLES) -> tuple:\n",
        "    \"\"\"Performs bootstrap resampling and returns the confidence interval, handling errors.\"\"\"\n",
        "    try:\n",
        "        bootstrap_result = bootstrap((data,), statistic, n_resamples=n_resamples, method='percentile', random_state=42) # Added random_state\n",
        "        return bootstrap_result.confidence_interval\n",
        "    except Exception as e:\n",
        "        print(f\"Error during bootstrap analysis: {e}\")\n",
        "        return (None, None)\n",
        "\n",
        "def save_summary(df: pd.DataFrame, bootstrap_ci: tuple, output_path: str) -> str:\n",
        "    \"\"\"Calculates and saves summary statistics and bootstrap CI, handling errors.\"\"\"\n",
        "    try:\n",
        "        summary_text = df.describe().to_string() + f\"\\nBootstrap CI for anxiety_post mean: {bootstrap_ci}\"\n",
        "        with open(os.path.join(output_path, 'summary.txt'), 'w') as f:\n",
        "            f.write(summary_text)\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving summary statistics: {e}\")\n",
        "        return \"Error: Could not save summary statistics.\"\n",
        "\n",
        "def perform_group_analysis(df: pd.DataFrame):\n",
        "    \"\"\"Performs statistical analysis by group, returning a detailed report.\"\"\"\n",
        "    try:\n",
        "        # Group-by-group analysis\n",
        "        group_stats = {}\n",
        "        for group in df['original_group'].unique():\n",
        "            group_df = df[df['original_group'] == group]\n",
        "            pre_mean = group_df[ANXIETY_PRE_COLUMN].mean()\n",
        "            post_mean = group_df[ANXIETY_POST_COLUMN].mean()\n",
        "            percent_change = ((post_mean - pre_mean) / pre_mean) * 100 if pre_mean > 0 else 0\n",
        "\n",
        "            group_stats[group] = {\n",
        "                'n': len(group_df),\n",
        "                'pre_mean': pre_mean,\n",
        "                'post_mean': post_mean,\n",
        "                'pre_std': group_df[ANXIETY_PRE_COLUMN].std(),\n",
        "                'post_std': group_df[ANXIETY_POST_COLUMN].std(),\n",
        "                'percent_change': percent_change\n",
        "            }\n",
        "\n",
        "        return group_stats\n",
        "    except Exception as e:\n",
        "        print(f\"Error in group analysis: {e}\")\n",
        "        return {}\n",
        "\n",
        "def generate_insights_report(summary_stats_text: str, shap_analysis_info: str,\n",
        "                            kde_plot_desc: str, violin_plot_desc: str,\n",
        "                            parallel_coords_desc: str, hypergraph_desc: str,\n",
        "                            group_stats: dict, output_path: str) -> None:\n",
        "    \"\"\"Generates an insights report based on statistical analysis, handling errors.\"\"\"\n",
        "    try:\n",
        "        # Generate insights based on data analysis rather than API calls\n",
        "        insights = f\"\"\"\n",
        "# Anxiety Intervention Analysis Report\n",
        "## Generated by {AUTHOR} on {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This report analyzes the effectiveness of an anxiety intervention study across different participant groups.\n",
        "The analysis is based on statistical findings and data visualizations including SHAP analysis,\n",
        "distribution plots, and relationship networks.\n",
        "\n",
        "## Key Statistical Findings\n",
        "\n",
        "### Summary Statistics\n",
        "{summary_stats_text}\n",
        "\n",
        "### Group-by-Group Analysis\n",
        "\"\"\"\n",
        "        # Add group stats to report\n",
        "        for group, stats in group_stats.items():\n",
        "            change_direction = \"decreased\" if stats['percent_change'] < 0 else \"increased\" if stats['percent_change'] > 0 else \"remained unchanged\"\n",
        "            insights += f\"\"\"\n",
        "#### {group} (n={stats['n']})\n",
        "- Pre-intervention anxiety (mean ± std): {stats['pre_mean']:.3f} ± {stats['pre_std']:.3f}\n",
        "- Post-intervention anxiety (mean ± std): {stats['post_mean']:.3f} ± {stats['post_std']:.3f}\n",
        "- Anxiety levels {change_direction} by {abs(stats['percent_change']):.2f}%\n",
        "\"\"\"\n",
        "\n",
        "        # Add SHAP analysis insights\n",
        "        insights += f\"\"\"\n",
        "## Feature Importance Analysis\n",
        "\n",
        "{shap_analysis_info}\n",
        "\n",
        "The SHAP analysis reveals which factors most strongly influence post-intervention anxiety levels,\n",
        "with pre-intervention anxiety levels and group assignment showing distinctive patterns of impact.\n",
        "\n",
        "## Visualization Insights\n",
        "\n",
        "- {kde_plot_desc}: The distribution comparison reveals overall patterns in anxiety levels before and after intervention.\n",
        "- {violin_plot_desc}: Shows the distribution of post-intervention anxiety levels across different groups.\n",
        "- {parallel_coords_desc}: Illustrates individual participant trajectories from pre to post intervention.\n",
        "- {hypergraph_desc}: Reveals relationship patterns between participants and anxiety levels.\n",
        "\n",
        "## Intervention Effectiveness\n",
        "\n",
        "\"\"\"\n",
        "        # Determine intervention effectiveness based on group stats\n",
        "        control_change = group_stats.get('Control', {}).get('percent_change', 0)\n",
        "        group_a_change = group_stats.get('Group A', {}).get('percent_change', 0)\n",
        "        group_b_change = group_stats.get('Group B', {}).get('percent_change', 0)\n",
        "\n",
        "        if group_a_change < control_change or group_b_change < control_change:\n",
        "            insights += \"\"\"\n",
        "The intervention shows promising results, with at least one intervention group demonstrating greater\n",
        "anxiety reduction compared to the control group. The changes appear to be statistically meaningful,\n",
        "though the limited sample size warrants caution in interpretation.\n",
        "\"\"\"\n",
        "        else:\n",
        "            insights += \"\"\"\n",
        "The intervention shows limited effectiveness, with intervention groups not demonstrating significantly\n",
        "greater anxiety reduction compared to the control group. Further investigation with a larger sample\n",
        "size may be necessary to detect more subtle effects.\n",
        "\"\"\"\n",
        "\n",
        "        insights += \"\"\"\n",
        "## Limitations and Future Directions\n",
        "\n",
        "1. **Sample Size**: The limited sample size restricts statistical power and generalizability.\n",
        "2. **Time Frame**: The analysis doesn't account for potential long-term effects or delayed responses.\n",
        "3. **Participant Variables**: Individual differences in baseline anxiety and responsiveness to intervention aren't fully explored.\n",
        "\n",
        "Future research should consider:\n",
        "- Larger sample sizes for more robust statistical analysis\n",
        "- Longitudinal follow-up to assess long-term intervention effects\n",
        "- Additional participant variables that might moderate intervention effectiveness\n",
        "- Qualitative assessment of participant experiences to complement quantitative findings\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This analysis provides initial insights into the effectiveness of the anxiety intervention. While statistical\n",
        "patterns have emerged, the results should be interpreted with appropriate caution given the limitations.\n",
        "The visualization techniques employed offer multiple perspectives on the data, enhancing our understanding\n",
        "of intervention effects and individual response patterns.\n",
        "\"\"\"\n",
        "\n",
        "        # Save the insights report\n",
        "        with open(os.path.join(output_path, 'insights_report.md'), 'w') as f:\n",
        "            f.write(insights)\n",
        "        print(f\"Insights report saved to: {os.path.join(output_path, 'insights_report.md')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating insights report: {e}\")\n",
        "        print(\"An error occurred, and the insights report could not be generated.\")\n",
        "\n",
        "# --- Main Script ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Create output directory\n",
        "    if not create_output_directory(OUTPUT_PATH):\n",
        "        exit()\n",
        "\n",
        "    # Synthetic dataset (small, embedded in code)\n",
        "    synthetic_dataset = \"\"\"\n",
        "participant_id,group,anxiety_pre,anxiety_post\n",
        "P001,Group A,4,2\n",
        "P002,Group A,3,1\n",
        "P003,Group A,5,3\n",
        "P004,Group B,6,5\n",
        "P005,Group B,5,4\n",
        "P006,Group B,7,6\n",
        "P007,Control,3,3\n",
        "P008,Control,4,4\n",
        "P009,Control,2,2\n",
        "P010,Control,5,5\n",
        "\"\"\"\n",
        "    # Load and validate data\n",
        "    df = load_data_from_synthetic_string(synthetic_dataset)\n",
        "    if df is None:\n",
        "        exit()\n",
        "\n",
        "    required_columns = [PARTICIPANT_ID_COLUMN, GROUP_COLUMN, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN]\n",
        "    is_valid, valid_groups = validate_dataframe(df, required_columns)\n",
        "    if not is_valid:\n",
        "        exit()\n",
        "\n",
        "    # --- Data Preprocessing ---\n",
        "\n",
        "    # Keep the original group for plots\n",
        "    df_original_group = df[GROUP_COLUMN].copy()\n",
        "\n",
        "    # One-hot encode, *without* dropping the first category\n",
        "    df = pd.get_dummies(df, columns=[GROUP_COLUMN], prefix=GROUP_COLUMN, drop_first=False)\n",
        "    encoded_group_cols = [col for col in df.columns if col.startswith(f\"{GROUP_COLUMN}_\")]\n",
        "\n",
        "    # Add back the original group (with a new name)\n",
        "    df['original_group'] = df_original_group\n",
        "\n",
        "    # Scale the data\n",
        "    df = scale_data(df, [ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN] + encoded_group_cols)\n",
        "    if df is None:\n",
        "        exit()\n",
        "\n",
        "    # --- SHAP Analysis ---\n",
        "    shap_feature_columns = encoded_group_cols + [ANXIETY_PRE_COLUMN]\n",
        "    shap_analysis_info = calculate_shap_values(df.copy(), shap_feature_columns, ANXIETY_POST_COLUMN, OUTPUT_PATH)\n",
        "\n",
        "    # --- Visualizations ---\n",
        "    # Define a consistent color palette\n",
        "    neon_colors = [\"#FF00FF\", \"#00FFFF\", \"#FFFF00\", \"#00FF00\"]\n",
        "\n",
        "    # Create static visualizations (Matplotlib/Seaborn)\n",
        "    kde_plot_desc = create_kde_plot(df, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors[:2])\n",
        "    violin_plot_desc = create_violin_plot(df, 'original_group', ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors)  # Use original group\n",
        "    parallel_coords_desc = create_parallel_coordinates_plot(df, 'original_group', ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors) # Use original group\n",
        "    hypergraph_desc = visualize_hypergraph(df, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors[:2])\n",
        "\n",
        "    # --- Statistical Analysis ---\n",
        "    bootstrap_ci = perform_bootstrap(df[ANXIETY_POST_COLUMN], np.mean)\n",
        "\n",
        "    # --- Group Analysis ---\n",
        "    group_stats = perform_group_analysis(df)\n",
        "\n",
        "    # --- Save Summary ---\n",
        "    summary_stats_text = save_summary(df, bootstrap_ci, OUTPUT_PATH)\n",
        "\n",
        "    # --- Generate Insights Report (without API calls) ---\n",
        "    generate_insights_report(summary_stats_text, shap_analysis_info, kde_plot_desc,\n",
        "                           violin_plot_desc, parallel_coords_desc, hypergraph_desc,\n",
        "                           group_stats, OUTPUT_PATH)\n",
        "\n",
        "    print(\"Execution completed successfully - Static Visualization Enhanced Notebook with integrated insights report.\")"
      ]
    }
  ]
}